# 100-Days-of-Machine-learning
100 Day ML Challenge to learn and implement ML/DL concepts ranging from the basics to more advanced state of the art models.
Finished machine learning concepts from Andrew NG's course by Standford University on Coursera.

# Daily logs:
## Day-1 [17-09-2021] Introduction:
* Started ["Machine learning- A Probabilistic Perspective" by Kevin Murphy.](http://noiselab.ucsd.edu/ECE228/Murphy_Machine_Learning.pdf)
* Introduction to machine learning- learnt about: Matrix completion, Image Inpainting, collaborative filtering, No free lunch theorem and Market Basket analysis.
* 4 Distance measures widely used in machine learning.

## Day-2 [18-09-2021] Regression Analysis:
* Started by solving a problem related to the previous day which involved KNN and MNIST Dataset.
* Learnt about different types of regression namely Linear Regression, Logistic Regression, Ridge Regression, Lasso Regression and Polynomial Regression.
* Each one with their Equations and Graphs

## Day-3 [19-09-2021] Support Vector Machines:
![image](https://user-images.githubusercontent.com/77164319/133924595-894fdf2d-5a38-4ecc-a165-0712ba97e39a.png)
* Understood the intuition behind SVMs.
* Implemented a simple classification model using [scikit-learn](https://scikit-learn.org/stable/) SVM for the [Bank-retirement](https://www.kaggle.com/adarshkumarjha/bank-customer-retirement) dataset available on Kaggle.

##  Day-4 [20-09-2021] Naive-Bayes:
![image](https://user-images.githubusercontent.com/77164319/134170787-74e88010-a8b4-495e-aee8-4a4d0dfb368e.png)

* Understood the intuition behind Naive Bayes Classifier.
* Implemented a simple Naive Bayes Classification model using scratch for [Iris dataset](https://www.kaggle.com/vikrishnan/iris-dataset) available on Kaggle.

## Day-5 [21-09-2021] Hyperparameter tuning:
* learnt the importance of hyperparameter tuning in machine learning algorithms.
* saw different parameters for SVMs, Naive Bayes and KNNs. 

## Day-6 [22-09-2021] Bias, Variance, Cross validation and confusion matrix:
* Learnt concepts Bias, Variance, Cross Validation, Confusion Matrix.
* Implemented GridSearchCV and selected the best hyperparamter for Support Vector machine on a dataset.
* saw the [stanford cheatsheet](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks) for basics of machine learning.

## Day-7 [24-09-2021] Numpy, Pandas, Matplotlib and Seaborn:
* Went through the documentation and implemented topics.
* [Pandas cheatsheet](http://datacamp-community-prod.s3.amazonaws.com/f04456d7-8e61-482f-9cc9-da6f7f25fc9b)
* [Matplotlib Cheatsheet](http://datacamp-community-prod.s3.amazonaws.com/e1a8f39d-71ad-4d13-9a6b-618fe1b8c9e9)
* [Seaborn Cheatsheet](http://datacamp-community-prod.s3.amazonaws.com/263130e2-2c92-4348-a356-9ed9b5034247)

## Day-8 [11-10-2021] Kernels:
* Learnt basics of kernal functions.
* Also started watching Stanford's CS299 lecture on [kernels](https://www.youtube.com/watch?v=8NYoQiRANpg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=10)

## Day-9 [12-10-2021] Kernels Continued:
* Learnt kernels for SVMs-polynomial and Radial Kernal(Radial bias function).
![image](https://user-images.githubusercontent.com/77164319/136872058-7ef4172d-f768-433f-b3f0-b0153eeed5e0.png)
* Learnt kernels and filters for convolution.
* Referred to chapter 14 in Machine learning-A Probabilistic perspective by Kevin Murphy.

## Day-10 [13-10-2021] Decision Trees:
* Learnt Basics of Decision Trees through numerous examples.
* learnt how to calculate gini index and other parameters.
* Watched Video by [StatQuest](https://www.youtube.com/watch?v=7VeUPuFGJHk) on Classification and regression Decision Trees.

## Day-11 [14-10-2021] Random Forest, Regression and Adaboost:
* Learnt the intuition behind random forest and adaboost.
* learnt concepts related to proximity matrix and distance matrix.
* How to combine different learning algorithms and average their results.
* Advantages and disadvantages of decision trees.
* Implemented and visualised a decision tree using [scikit learn](https://scikit-learn.org/stable/) and [seaborn](https://seaborn.pydata.org/).
* Learnt how to prune regresstion trees using the residual sum of squares and tree score.
* Learnt about the ID3 algorithm, Entropy and Information gain.
![image](https://user-images.githubusercontent.com/77164319/137328976-39dcfb02-476f-405e-9fd4-5c6b1c366fbd.png)

